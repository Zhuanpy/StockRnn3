#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯¦ç»†åˆ†æåŸºé‡‘é¡µé¢ç»“æ„
"""

import sys
import os
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def analyze_fund_page(fund_code="003069"):
    """è¯¦ç»†åˆ†æåŸºé‡‘é¡µé¢ç»“æ„"""
    try:
        print(f"ğŸ” è¯¦ç»†åˆ†æåŸºé‡‘é¡µé¢: {fund_code}")
        print("=" * 60)
        
        from App.codes.downloads.download_utils import page_source
        from config import Config
        from bs4 import BeautifulSoup as soup
        
        url = Config.get_eastmoney_urls('funds_awkward').format(fund_code)
        headers = Config.get_eastmoney_headers('funds_awkward')
        
        print(f"URL: {url}")
        
        # è·å–é¡µé¢æºç 
        source = page_source(url=url, headers=headers)
        
        if not source:
            print("âŒ é¡µé¢æºç ä¸ºç©º")
            return False
        
        print(f"âœ… é¡µé¢æºç é•¿åº¦: {len(source)}")
        
        # ä¿å­˜é¡µé¢æºç åˆ°æ–‡ä»¶
        debug_file = f"debug_fund_{fund_code}.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(source)
        print(f"ğŸ’¾ é¡µé¢æºç å·²ä¿å­˜åˆ°: {debug_file}")
        
        # è§£æHTML
        soup_obj = soup(source, 'html.parser')
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        print(f"\nğŸ”— åˆ†ææ‰€æœ‰é“¾æ¥...")
        all_links = soup_obj.find_all("a")
        print(f"æ€»é“¾æ¥æ•°é‡: {len(all_links)}")
        
        # åˆ†ç±»é“¾æ¥
        stock_links = []
        fund_links = []
        other_links = []
        
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            if '/stock/' in href:
                stock_links.append((text, href))
            elif '/fund/' in href or fund_code in href:
                fund_links.append((text, href))
            else:
                other_links.append((text, href))
        
        print(f"è‚¡ç¥¨ç›¸å…³é“¾æ¥: {len(stock_links)}")
        print(f"åŸºé‡‘ç›¸å…³é“¾æ¥: {len(fund_links)}")
        print(f"å…¶ä»–é“¾æ¥: {len(other_links)}")
        
        # æ˜¾ç¤ºè‚¡ç¥¨é“¾æ¥
        if stock_links:
            print("\nğŸ“ˆ è‚¡ç¥¨é“¾æ¥:")
            for i, (text, href) in enumerate(stock_links[:10]):
                print(f"  {i+1}. {text} -> {href}")
        else:
            print("\nâŒ æœªæ‰¾åˆ°è‚¡ç¥¨é“¾æ¥")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        print(f"\nğŸ“Š åˆ†æè¡¨æ ¼ç»“æ„...")
        tables = soup_obj.find_all("table")
        print(f"è¡¨æ ¼æ•°é‡: {len(tables)}")
        
        for i, table in enumerate(tables):
            print(f"\n--- è¡¨æ ¼ {i+1} ---")
            
            # æŸ¥æ‰¾è¡¨å¤´
            thead = table.find("thead")
            if thead:
                headers = [th.get_text(strip=True) for th in thead.find_all(['th', 'td'])]
                print(f"è¡¨å¤´: {headers}")
            
            # æŸ¥æ‰¾è¡¨ä½“
            tbody = table.find("tbody")
            if tbody:
                rows = tbody.find_all("tr")
                print(f"è¡Œæ•°: {len(rows)}")
                
                # åˆ†æå‰å‡ è¡Œ
                for j, row in enumerate(rows[:3]):
                    tds = row.find_all('td')
                    td_texts = [td.get_text(strip=True) for td in tds]
                    print(f"  è¡Œ {j+1}: {td_texts}")
            else:
                print("æ— tbody")
        
        # 3. æŸ¥æ‰¾JavaScriptæ•°æ®
        print(f"\nğŸ” æŸ¥æ‰¾JavaScriptæ•°æ®...")
        
        # æŸ¥æ‰¾scriptæ ‡ç­¾
        scripts = soup_obj.find_all("script")
        print(f"Scriptæ ‡ç­¾æ•°é‡: {len(scripts)}")
        
        # æŸ¥æ‰¾å¯èƒ½åŒ…å«è‚¡ç¥¨æ•°æ®çš„script
        for i, script in enumerate(scripts):
            script_content = script.string
            if script_content:
                # æŸ¥æ‰¾åŒ…å«è‚¡ç¥¨ä»£ç çš„å†…å®¹
                if 'stock' in script_content.lower() or 'position' in script_content.lower():
                    print(f"\n--- Script {i+1} ---")
                    print(f"å†…å®¹é•¿åº¦: {len(script_content)}")
                    print(f"å†…å®¹é¢„è§ˆ: {script_content[:500]}...")
                    
                    # æŸ¥æ‰¾6ä½æ•°å­—ï¼ˆå¯èƒ½çš„è‚¡ç¥¨ä»£ç ï¼‰
                    numbers = re.findall(r'\b\d{6}\b', script_content)
                    if numbers:
                        print(f"æ‰¾åˆ°çš„æ•°å­—: {numbers[:10]}")
        
        # 4. æŸ¥æ‰¾ç‰¹å®šclasså’Œid
        print(f"\nğŸ¯ æŸ¥æ‰¾ç‰¹å®šå…ƒç´ ...")
        
        # æŸ¥æ‰¾åŒ…å«"æŒä»“"çš„å…ƒç´ 
        position_elements = soup_obj.find_all(text=lambda text: text and 'æŒä»“' in text)
        print(f"åŒ…å«'æŒä»“'çš„å…ƒç´ : {len(position_elements)}")
        
        # æŸ¥æ‰¾åŒ…å«"è‚¡ç¥¨"çš„å…ƒç´ 
        stock_elements = soup_obj.find_all(text=lambda text: text and 'è‚¡ç¥¨' in text)
        print(f"åŒ…å«'è‚¡ç¥¨'çš„å…ƒç´ : {len(stock_elements)}")
        
        # æŸ¥æ‰¾å¯èƒ½çš„æŒä»“ç›¸å…³class
        position_classes = ['position', 'holdings', 'stock', 'portfolio']
        for class_name in position_classes:
            elements = soup_obj.find_all(class_=re.compile(class_name, re.I))
            if elements:
                print(f"classåŒ…å«'{class_name}'çš„å…ƒç´ : {len(elements)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_multiple_funds():
    """æµ‹è¯•å¤šä¸ªåŸºé‡‘"""
    test_funds = ["003069", "002556", "001072"]
    
    for fund_code in test_funds:
        print(f"\n{'='*80}")
        print(f"åˆ†æåŸºé‡‘: {fund_code}")
        print(f"{'='*80}")
        
        if not analyze_fund_page(fund_code):
            print(f"åŸºé‡‘ {fund_code} åˆ†æå¤±è´¥")
        
        print("\n" + "-"*80)

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è¯¦ç»†åˆ†æåŸºé‡‘é¡µé¢ç»“æ„...")
    
    # åˆ†æå•ä¸ªåŸºé‡‘
    analyze_fund_page("003069")
    
    # æˆ–è€…åˆ†æå¤šä¸ªåŸºé‡‘
    # test_multiple_funds() 